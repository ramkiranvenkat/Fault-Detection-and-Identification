# -*- coding: utf-8 -*-
"""FDISeqLearnLSTM (100).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AKwehztpdtKKensPAnPBVPIHI3Gs94FN
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import urllib
import sys
import os
import zipfile
import torch.nn as nn
import torch.nn.functional as F
import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader, random_split

import torchvision
from torchvision.datasets import FashionMNIST
from torchvision import transforms
from google.colab import drive
drive.mount('/content/drive')

zip_ref = zipfile.ZipFile("/content/drive/My Drive/SeqData.zip", 'r')
zip_ref.extractall("/home")
zip_ref.close()

!ls /home/ReadDataFolder

import pandas as pd
path = "/home/ReadDataFolder/"
def rearrangeData(dp):
  a = []
  for i in range(dp.shape[0]):
    a.append(dp[i,2:])
  return np.array(a)

def extractdata(fname):
  data =  pd.read_csv(path + fname,sep=' ',header=None)
  dataPoints = rearrangeData(data.to_numpy())
  return dataPoints


normal100 = []
normal300 = []
normal1500= []
failed100 = []
failed300 = [] 
failed1500= []

normal100Type = []
normal300Type = []
normal1500Type = []
failed100Type = []
failed300Type = [] 
failed1500Type = []

for filename in os.listdir(path):
  if filename[-2] == 'N':
    datap = extractdata(filename)
    if datap.shape[0] == 100:
      normal100.append(datap)
      normal100Type.append(filename[-2:])
    elif datap.shape[0] == 300:
      normal300.append(datap)
      normal300Type.append(filename[-2:])
    else:
      normal1500.append(datap)
      normal1500Type.append(filename[-2:])
  else:
    datap = extractdata(filename)
    if datap.shape[0] == 100:
      failed100.append(datap)
      failed100Type.append(filename[-2:])
    elif datap.shape[0] == 300:
      failed300.append(datap)
      failed300Type.append(filename[-2:])
    else:
      failed1500.append(datap)
      failed1500Type.append(filename[-2:])

normal100 = np.array(normal100)
failed100 = np.array(failed100)
normal300 = np.array(normal300)
failed300 = np.array(failed300)
normal1500= np.array(normal1500)
failed1500= np.array(failed1500)

seqlength = 100
overlap = 1.0
dataCollection = []
dataType = []
dataCollectionTest = []
dataTypeTest = []

for i in range(normal300.shape[0]):
  ldata = normal300[i][1:]
  ldata[1:,11:] = ldata[1:,11:]-ldata[:-1,11:]
  ltype = normal300Type[i]
  for j in range(0,ldata.shape[0]-seqlength,int(seqlength*overlap)):
    dataCollection.append(ldata[j:j+seqlength][:])
    dataType.append(ltype)

for i in range(normal1500.shape[0]):
  ldata = normal1500[i][1:]
  ldata[1:,11:] = ldata[1:,11:]-ldata[:-1,11:]
  ltype = normal1500Type[i]
  for j in range(0,ldata.shape[0]-seqlength,int(seqlength*overlap)):
    dataCollection.append(ldata[j:j+seqlength][:])
    dataType.append(ltype)

for i in range(failed300.shape[0]):
  ldata = failed300[i][1:]
  ldata[1:,11:] = ldata[1:,11:]-ldata[:-1,11:]
  ltype = failed300Type[i]
  for j in range(0,ldata.shape[0]-seqlength,int(seqlength*overlap)):
    dataCollectionTest.append(ldata[j:j+seqlength][:])
    dataTypeTest.append(ltype)

for i in range(failed1500.shape[0]):
  ldata = failed1500[i][1:]
  ldata[1:,11:] = ldata[1:,11:]-ldata[:-1,11:]
  ltype = failed1500Type[i]
  for j in range(0,ldata.shape[0]-seqlength,int(seqlength*overlap)):
    dataCollectionTest.append(ldata[j:j+seqlength][:])
    dataTypeTest.append(ltype)

dataCollection = np.array(dataCollection)
dataCollectionTest = np.array(dataCollectionTest)
print(len(dataType))
print(len(dataTypeTest))
print(dataCollection.shape)

plt.plot(dataCollection[2,:,11])
plt.show()

idx = np.random.permutation(dataCollection.shape[0])
dataCollectionShuffeled = dataCollection[idx]
print(dataCollection.shape)
print(dataCollectionTest.shape)

N = int(0.8*dataCollection.shape[0])
dataPointsT = dataCollection[:N]

idx = np.random.permutation(dataPointsT.shape[0])
dataPoints = dataPointsT[idx]

dpt = dataCollection[N:]
dataPointsTest = np.vstack((dataCollectionTest,dpt))
print(dataPoints.shape)
print(dataPointsTest.shape)

# Data Normalization
print(dataPoints[:,0,:].shape)
maxvals = np.amax(dataPoints,axis=0)
minvals = np.amin(dataPoints,axis=0)
maxvals = np.amax(maxvals,axis=0)
minvals = np.amin(minvals,axis=0)

maxvalsT = np.amax(dataPointsTest,axis=0)
minvalsT = np.amin(dataPointsTest,axis=0)
maxvalsT = np.amax(maxvalsT,axis=0)
minvalsT = np.amin(minvalsT,axis=0)

maxvals = np.maximum(maxvals,maxvalsT)
minvals = np.minimum(minvals,minvalsT)
print(maxvals,minvals)

difvals = maxvals-minvals

dataPointsNormalised = dataPoints.copy()
dataPointsTestNormalised = dataPointsTest.copy()

for i in range(dataPoints.shape[0]):
  for j in range(dataPoints.shape[1]):
    dataPointsNormalised[i,j,:] = np.divide(dataPoints[i,j,:] - minvals, difvals)
for i in range(dataPointsTest.shape[0]):
  for j in range(dataPointsTest.shape[1]):
    dataPointsTestNormalised[i,j,:] = np.divide(dataPointsTest[i,j,:] - minvals, difvals)

print(dataPointsNormalised.shape)
np.amax(np.amax(dataPointsNormalised,axis=0),axis=0)
np.amin(np.amin(dataPointsNormalised,axis=0),axis=0)

print(dataPointsNormalised.shape)
print(dataPointsTestNormalised.shape)

def ShapeAdjustment(seqs):
  l = seqs.shape[0]
  datuml = seqs.shape[2]
  seqSA = np.zeros((seqs.shape[1],l,datuml)) # shape adjusted (SA)
  for _ in range(l):
    temp = seqs[_,:,:]
    seqSA[:,_,:] = temp;
  return torch.from_numpy(seqSA).float()

X_train = ShapeAdjustment(dataPointsNormalised)
print(X_train.shape)

X_test = ShapeAdjustment(dataPointsTestNormalised)
print(X_test.shape)

#np.save('X_test_seq.npy', X_test) # save
#np.save('X_train_seq.npy', X_train)
torch.save(X_train, 'X_train.pt')
torch.save(X_test, 'X_test.pt')

X_train = torch.load('/content/drive/My Drive/100sTS/X_train_100.pt') # load
X_test = torch.load('/content/drive/My Drive/100sTS/X_test_100.pt')

print(X_train.shape,X_test.shape)

import torch.nn as nn
import torch.nn.functional as F
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class InitNetwork(nn.Module):
  def __init__(self, input_size, output_size):
    super(InitNetwork, self).__init__()
    self.dense1 = nn.Linear(input_size, 256)
    self.dense2 = nn.Linear(256, 128)
    self.dense3 = nn.Linear(128, 64)
    self.dense4 = nn.Linear(64, output_size)
  
  def forward(self,input):
    x = F.relu(self.dense1(input))
    x = F.relu(self.dense2(x))
    x = F.relu(self.dense3(x))
    x = torch.sigmoid(self.dense4(x))
    return x

class InitNetwork_small(nn.Module):
  def __init__(self, input_size, output_size):
    super(InitNetwork_small, self).__init__()
    self.dense1 = nn.Linear(input_size, 64)
    self.dense2 = nn.Linear(64, 32)
    self.dense3 = nn.Linear(32, 24)
    self.dense4 = nn.Linear(24, output_size)
  
  def forward(self,input):
    x = F.relu(self.dense1(input))
    x = F.relu(self.dense2(x))
    x = F.relu(self.dense3(x))
    x = torch.sigmoid(self.dense4(x))
    return 

class dynamics(nn.Module):
  def __init__(self, input_size, hidden_size,NOLSTM):
    super(dynamics, self).__init__()
    self.nlstm = NOLSTM
    self.hidden_size = hidden_size
    self.lstm = nn.LSTM(input_size, hidden_size, NOLSTM).to(device)
    self.hNet = []
    self.cNet = []
    for i in range(NOLSTM):
      self.hNet.append(InitNetwork(input_size,hidden_size).to(device))
      self.cNet.append(InitNetwork(input_size,hidden_size).to(device))
    
    self.oNet = InitNetwork(hidden_size,input_size).to(device)

  def forward(self, inputs):
    
    inp = None
    init_flag = True
    ret_out = torch.zeros_like(inputs)
    ret_h = torch.zeros(inputs.shape[0],self.nlstm,inputs.shape[1],self.hidden_size)
    ret_c = torch.zeros(inputs.shape[0],self.nlstm,inputs.shape[1],self.hidden_size)
    ret_hn = torch.zeros(inputs.shape[0],self.nlstm,inputs.shape[1],self.hidden_size)
    ret_cn = torch.zeros(inputs.shape[0],self.nlstm,inputs.shape[1],self.hidden_size)
    
    itr = 0
    h = torch.zeros(self.nlstm,inputs.shape[1],self.hidden_size)
    c = torch.zeros(self.nlstm,inputs.shape[1],self.hidden_size)
    hn = torch.zeros(self.nlstm,inputs.shape[1],self.hidden_size)
    cn = torch.zeros(self.nlstm,inputs.shape[1],self.hidden_size)
    
    for iseq in inputs:
      
      for j in range(self.nlstm):
        hn[j] = self.hNet[j](iseq).reshape(1,inputs.shape[1],self.hidden_size)
        cn[j] = self.cNet[j](iseq).reshape(1,inputs.shape[1],self.hidden_size)
      
      if (init_flag):
        inp = iseq.reshape(1,iseq.shape[0],iseq.shape[1])
        h = hn.clone().detach()
        c = cn.clone().detach()
        init_flag = False
      
      ret_h[itr] = h.clone().detach()
      ret_c[itr] = c.clone().detach()
      ret_hn[itr] = hn
      ret_cn[itr] = cn
      output, (h,c) = self.lstm(inp, (h.to(device),c.to(device)))
      inp = self.oNet(output)  
      ret_out[itr] = inp
      itr+=1  
    return ret_out, ret_h, ret_c, ret_hn, ret_cn

class dynamics_only_lstm(nn.Module):
  def __init__(self, input_size, hidden_size,NOLSTM):
    super(dynamics_only_lstm, self).__init__()
    self.nlstm = NOLSTM
    self.hidden_size = hidden_size
    self.lstm = nn.LSTM(input_size, hidden_size, NOLSTM)
    self.oNet = InitNetwork(hidden_size,input_size)

  def forward(self, inputs):
    # print(inputs.shape)
    output, (h,c) = self.lstm(inputs)
    output = self.oNet(output)
    # print(output.shape)
    return output, h, c

print(device)
n_hidden = 512
gamma = 0
#dyn = dynamics_only_lstm(19, n_hidden, 3).to(device)
n_hidden = 64
dyn = dynamics(19, n_hidden, 3).to(device)
dyn.load_state_dict(torch.load('dynamics100.pt',map_location=torch.device(device)))
criterionL1 = nn.L1Loss()
criterionMSE = nn.MSELoss()
learning_rate = 0.001
optimizer = torch.optim.Adam(dyn.parameters(), lr=learning_rate)

def TrainBatch(Xin,gamma):
  
  o, h, c, hn, cn = dyn(Xin)
  
  #o, h, c = dyn(Xin)
  loss = criterionMSE(o[:-1],Xin[1:])  + gamma*(criterionMSE(h,hn) + criterionMSE(c,cn))
  optimizer.zero_grad()
  ll = loss.item()
  loss.backward()
  optimizer.step()
  return ll

import tqdm
batch_size =4096
avg_loss = []
epochs = 400
val_loss = []
for epoch in range(epochs):
  loss = 0
  n = 0
  for i in range(0,X_train.shape[1], batch_size):
    ll = i
    hl = min(i+batch_size,X_train.shape[1])
    batch_data = X_train[:,ll:hl,:].to(device)
    loss+=TrainBatch(batch_data,gamma)
    n+=1
  if (epoch%10 == 1):
    torch.save(dyn.state_dict(), 'dynamics.pt')
    !cp -r dynamics.pt "/content/gdrive/My Drive/"
  avg_loss.append(loss/n)
  print("Epoch: ",epoch,"Loss: ",loss/n)

torch.save(dyn.state_dict(), 'dynamics100.pt')
avg_loss = np.array(avg_loss)
plt.plot(avg_loss)
plt.show()

with torch.no_grad():
  la = []
  for i in range(X_test.shape[1]):
    Xin = X_test[:,i,:].reshape(X_test.shape[0],1,X_test.shape[2]).to(device)
    o, h, c, hn, cn = dyn(Xin)
    loss = criterionMSE(o[:-1],Xin[1:])  + 0*(criterionMSE(h,hn) + criterionMSE(c,cn))
    ll = loss.item()
    la.append(ll)
  la = np.array(la)

with torch.no_grad():
  la = []
  o, h,c,hn,cn = dyn(X_test.to(device))
  for i in range(X_test.shape[1]):
    loss = criterionMSE(o[:,i,:].cpu(),X_test[:,i,:])
    ll = loss.item()
    la.append(ll)
  la = np.array(la)

def plot_hist(b,h,b1,h1,figno,name):
  b = (b[1:]+b[:-1])/2
  b1 = (b1[1:]+b1[:-1])/2
  print(b.shape,h.shape)
  plt.figure(figno)
  plt.plot(b,h,lw=2,label='Fail')
  plt.plot(b1,h1,lw=2,label='Pass')
  plt.legend()
  plt.title(name)
  plt.grid()
  plt.show()

def get_row(y,l):
  i1 = 0
  i2 = 0
  for i in range(y.shape[0]):
    if (y[i] < l):
      i1 += 1
    else:
      i2 += 1
  return i1, i2

def roc_data(p,n):
  print(p,n)
  max_level = max(max(p),max(n))
  min_level = min(min(p),min(n))
  tpr = [0]
  fpr = [0]
  eps = 1e-6
  acc = 0
  for i in range(1000):
    level = min_level + i*(max_level-min_level)/1000.0
    tp,fn = get_row(p,level)
    fp,tn = get_row(n,level)
    if (tp+tn)/(tp+tn+fn+fp) > acc:
      acc = (tp+tn)/(tp+tn+fn+fp)
    tpr.append(tp/(tp+fn + eps))
    fpr.append(fp/(fp+tn + eps))

  print("accuracy: ", acc)
  return tpr, fpr

plt.plot(la)
Nf = 14414
print(Nf,len(la))
tpr, fpr = roc_data(la[Nf:],la[:Nf])
fpr.append(1)
tpr.append(1)
lw = 2
plt.xlim([0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('Receiver operating characteristic', fontsize=16)

plt.plot(fpr, tpr, label='LSTM')
plt.plot([0, 1], [0, 1], color='navy', label='No Learning', lw=lw, linestyle='--')
plt.legend(loc="lower right", fontsize=12)

np.save('lstmDyn_fpr(100)',np.array(fpr))
np.save('lstmDyn_tpr(100)',np.array(tpr))

Nf = 14414
print(la.shape)

def plot_hist(b,h,b1,h1,figno,name):
  b = (b[1:]+b[:-1])/2
  b1 = (b1[1:]+b1[:-1])/2
  print(b.shape,h.shape)
  plt.figure(figno)
  plt.plot(b,h,lw=2,label='Fail')
  plt.plot(b1,h1,lw=2,label='Pass')
  plt.legend()
  plt.title(name)
  plt.grid()
  plt.show()


fsm_hist,fsm_bins = np.histogram(la[:Nf],bins = 100)
psm_hist,psm_bins = np.histogram(la[Nf:],bins = 100)

plot_hist(fsm_bins,fsm_hist/Nf,psm_bins,psm_hist/(21448-Nf),0,'LSTM')