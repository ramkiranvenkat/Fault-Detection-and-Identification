# -*- coding: utf-8 -*-
"""FDI_AED_DEC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yke3TQPgx0ePe83zDR7TxGgvwyEJNblR
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import urllib
import sys
import os
import zipfile
import torch.nn as nn
import torch.nn.functional as F
import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader, random_split

import torch.nn as nn
import torch.nn.functional as F
import torch

import torchvision
from torchvision.datasets import FashionMNIST
from torchvision import transforms
from google.colab import drive
drive.mount('/content/drive')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

X_train = torch.load('/content/drive/My Drive/X_train.pt') # load
X_test = torch.load('/content/drive/My Drive/X_test.pt')

print(X_train.shape,X_test.shape)

class Network(nn.Module):
  def __init__(self, input_size, output_size):
    super(Network, self).__init__()
    self.dense = nn.Linear(input_size, output_size)

  def forward(self,input):
    x = torch.sigmoid(self.dense(input))
    return x

class lstm_AED(nn.Module):
  def __init__(self, input_size, hidden_size,NOLSTM):
    super(lstm_AED, self).__init__()
    self.nlstm = NOLSTM
    self.hidden_size = hidden_size
    self.input_size = input_size

    self.Encoder = nn.LSTM(input_size, hidden_size, NOLSTM).to(device)
    self.Decoder = nn.LSTM(hidden_size, hidden_size, NOLSTM).to(device)
    self.DeOutToInp = Network(hidden_size,input_size).to(device)

  def forward(self, inputs):
    ret_out = torch.zeros_like(inputs)
    features, (h,c) = self.Encoder(inputs)
    f = features[-1].reshape(1,features.shape[1],features.shape[2])

    

    h = torch.zeros(self.nlstm,inputs.shape[1],self.hidden_size).to(device)
    c = torch.zeros(self.nlstm,inputs.shape[1],self.hidden_size).to(device)
    for i in range(inputs.shape[0]):
      f, (h, c) = self.Decoder(f,(h, c))
      ret_out[i] = self.DeOutToInp(f)
    return ret_out, features[-1]

n_hidden = 10
aed = lstm_AED(19, n_hidden, 3).to(device)
aed.load_state_dict(torch.load('aed.pt',map_location=torch.device('cuda')))
criterionL1 = nn.L1Loss()
criterionMSE = nn.MSELoss()
learning_rate = 0.001
optimizer = torch.optim.Adam(aed.parameters(), lr=learning_rate)

def TrainBatch(Xin):
  o, f = aed(Xin)
  loss = criterionMSE(o,Xin)
  optimizer.zero_grad()
  ll = loss.item()
  loss.backward()
  optimizer.step()
  return ll

import tqdm
batch_size =4096
avg_loss = []
epochs = 400
val_loss = []
for epoch in range(epochs):
  loss = 0
  n = 0
  for i in range(0,X_train.shape[1], batch_size):
    ll = i
    hl = min(i+batch_size,X_train.shape[1])
    batch_data = X_train[:,ll:hl,:].to(device)
    loss+=TrainBatch(batch_data)
    n+=1
  if (epoch%10 == 1):
    torch.save(aed.state_dict(), 'aed.pt')
  avg_loss.append(loss/n)
  print("Epoch: ",epoch,"Loss: ",loss/n)

torch.save(aed.state_dict(), 'aed.pt')
plt.plot(avg_loss)

from sklearn.cluster import KMeans
n_cluster = 100
n_dim = 2

with torch.no_grad():
  o, f = aed(X_train.to(device))
  kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(f.cpu().numpy())
  mus = torch.from_numpy(kmeans.cluster_centers_).to(device).detach()

Qv = Q_comp(mus,f,n_cluster,1)
Pv = P_comp(Qv)

print(Qv.shape,Pv.shape)
l,dm = cluster_loss(mus,f,n_cluster,1)
print(l,dm.shape,Qv.requires_grad,Pv.requires_grad)

n_hidden = 10
aed_cls = lstm_AED(19, n_hidden, 3).to(device)
aed_cls.load_state_dict(torch.load('aed_cls.pt',map_location=torch.device(device)))
criterionL1 = nn.L1Loss()
criterionMSE = nn.MSELoss()
learning_rate = 0.001
optimizer = torch.optim.Adam(aed_cls.parameters(), lr=learning_rate)

def Q_comp(mu,f,n_cluster,alpha):
  sz = f.shape[0]
  Q = torch.zeros(sz,n_cluster)
  for i in range(sz):
    Q[i] = torch.pow(1+torch.square(torch.norm(mu-f[i][None,:],dim=1))/alpha,-(alpha+1)/2)/torch.sum(torch.pow(1+torch.square(torch.norm(mu-f[i][None,:],dim=1))/alpha,-(alpha+1)/2))
  return Q

def P_comp(Q):
  Pt = torch.square(Q)
  P = torch.zeros_like(Q.clone())
  sm = torch.sum(Q,1)
  for i in range(P.shape[0]):
    P[i] = (Pt[i]/sm[i])/torch.sum(Pt[i]/sm[i])
  return P

def cluster_loss(mu,f,n_cluster,alpha):
  Qv = Q_comp(mu,f,n_cluster,alpha).to(device)
  Pv = P_comp(Qv).to(device)
  dmu = torch.zeros_like(mu).to(device)
  for j in range(n_cluster):
    dmu[j] = (alpha+1)/alpha*torch.sum((torch.pow(1+torch.square(torch.norm(f.detach()-mu[j][None,:],dim=1))/alpha,-1)*(Qv[:,j].detach()-Pv[:,j].detach()))[:,None]*(f.detach()-mu[j][None,:]))
  return torch.sum(Pv*(Pv/Qv).log()), dmu

def TrainBatch_cls(Xin,mu):
  o, f = aed_cls(Xin)
  cls_loss, mu_update = cluster_loss(mu,f,n_cluster,1)
  print(cls_loss)
  loss = criterionMSE(o,Xin) + 0.1*cls_loss
  mu += learning_rate*mu_update
  optimizer.zero_grad()
  ll = loss.item()
  loss.backward()
  optimizer.step()
  return ll, mu

batch_size =4096
avg_loss = []
epochs = 100
val_loss = []
torch.autograd.set_detect_anomaly(True)
for epoch in range(epochs):
  loss = 0
  n = 0
  for i in range(0,X_train.shape[1], batch_size):
    ll = i
    hl = min(i+batch_size,X_train.shape[1])
    batch_data = X_train[:,ll:hl,:].to(device)
    lloss, mus = TrainBatch_cls(batch_data,mus)
    loss+=lloss
    n+=1
  if (epoch%10 == 1):
    torch.save(aed_cls.state_dict(), 'aed_cls.pt')
  avg_loss.append(loss/n)
  print("Epoch: ",epoch,"Loss: ",loss/n)

torch.save(aed_cls.state_dict(), 'aed_cls.pt')
plt.plot(avg_loss)